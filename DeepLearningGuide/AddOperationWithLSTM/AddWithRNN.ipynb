{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '+': 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11}\n",
      "{0: ' ', 1: '+', 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9'}\n",
      "Generating data...\n",
      "['    5+0', '   4+06', '   5+86', '   36+1', '  687+1', '  71+81', '   6+98', '   9+78', '789+608', '  2+759']\n",
      "['5   ', '64  ', '73  ', '64  ', '787 ', '35  ', '95  ', '96  ', '1793', '959 ']\n",
      "Total questions: 50000\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable:\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        print(self.char_indices)\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        print(self.indices_char)\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+ \"\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print(\"Generating data...\")\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(\n",
    "        \"\".join(\n",
    "            np.random.choice(list(\"0123456789\"))\n",
    "            for i in range(np.random.randint(1, DIGITS + 1))\n",
    "        )\n",
    "    )\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = \"{}+{}\".format(a, b)\n",
    "    query = q + \" \" * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print(questions[0:10])\n",
    "print(expected[0:10])\n",
    "print(\"Total questions:\", len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorization...\")\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Validation Data:\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False False False False False False False False False False False]\n",
      " [False False False False False False False False  True False False False]\n",
      " [False False False False  True False False False False False False False]\n",
      " [False  True False False False False False False False False False False]\n",
      " [False False False False  True False False False False False False False]\n",
      " [False False False False False False  True False False False False False]\n",
      " [False False False False False False False  True False False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Build model...\")\n",
    "num_layers = 1  # Try to add more LSTM layers!\n",
    "\n",
    "model = keras.Sequential()\n",
    "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(num_layers):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 1.7692 - accuracy: 0.3533 - val_loss: 1.5769 - val_accuracy: 0.4053\n",
      "Q 55+32   T 87   ☒ 44  \n",
      "Q 816+17  T 833  ☒ 880 \n",
      "Q 505+42  T 547  ☒ 556 \n",
      "Q 165+600 T 765  ☒ 710 \n",
      "Q 6+867   T 873  ☒ 876 \n",
      "Q 212+7   T 219  ☒ 221 \n",
      "Q 9+518   T 527  ☒ 164 \n",
      "Q 532+4   T 536  ☒ 446 \n",
      "Q 6+323   T 329  ☒ 346 \n",
      "Q 297+7   T 304  ☒ 100 \n",
      "\n",
      "Iteration 2\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 1.3470 - accuracy: 0.4950 - val_loss: 1.1499 - val_accuracy: 0.5745\n",
      "Q 704+262 T 966  ☒ 1024\n",
      "Q 0+212   T 212  ☒ 225 \n",
      "Q 4+121   T 125  ☒ 128 \n",
      "Q 427+6   T 433  ☒ 435 \n",
      "Q 383+81  T 464  ☒ 455 \n",
      "Q 734+211 T 945  ☒ 905 \n",
      "Q 81+6    T 87   ☒ 80  \n",
      "Q 116+43  T 159  ☒ 158 \n",
      "Q 61+901  T 962  ☒ 954 \n",
      "Q 312+31  T 343  ☒ 335 \n",
      "\n",
      "Iteration 3\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 1.0159 - accuracy: 0.6258 - val_loss: 0.9197 - val_accuracy: 0.6635\n",
      "Q 928+49  T 977  ☒ 974 \n",
      "Q 58+298  T 356  ☒ 354 \n",
      "Q 104+76  T 180  ☒ 174 \n",
      "Q 590+17  T 607  ☒ 613 \n",
      "Q 979+8   T 987  ☒ 984 \n",
      "Q 43+62   T 105  ☒ 10  \n",
      "Q 424+42  T 466  ☒ 467 \n",
      "Q 206+431 T 637  ☒ 643 \n",
      "Q 2+915   T 917  ☒ 924 \n",
      "Q 42+250  T 292  ☒ 294 \n",
      "\n",
      "Iteration 4\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.8455 - accuracy: 0.6903 - val_loss: 0.8115 - val_accuracy: 0.6946\n",
      "Q 504+204 T 708  ☒ 829 \n",
      "Q 236+162 T 398  ☒ 494 \n",
      "Q 421+131 T 552  ☒ 559 \n",
      "Q 470+114 T 584  ☒ 581 \n",
      "Q 62+836  T 898  ☒ 993 \n",
      "Q 529+34  T 563  ☒ 561 \n",
      "Q 66+654  T 720  ☒ 722 \n",
      "Q 67+450  T 517  ☒ 519 \n",
      "Q 62+896  T 958  ☒ 960 \n",
      "Q 447+25  T 472  ☒ 479 \n",
      "\n",
      "Iteration 5\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 0.7469 - accuracy: 0.7302 - val_loss: 0.7220 - val_accuracy: 0.7304\n",
      "Q 908+81  T 989  ☒ 986 \n",
      "Q 730+812 T 1542 ☒ 1547\n",
      "Q 37+326  T 363  ☒ 369 \n",
      "Q 833+777 T 1610 ☒ 1616\n",
      "Q 372+940 T 1312 ☒ 1316\n",
      "Q 639+94  T 733  ☒ 731 \n",
      "Q 37+557  T 594  ☒ 596 \n",
      "Q 10+632  T 642  ☒ 638 \n",
      "Q 5+451   T 456  ☒ 454 \n",
      "Q 155+8   T 163  ☒ 161 \n",
      "\n",
      "Iteration 6\n",
      "1407/1407 [==============================] - 6s 5ms/step - loss: 0.6292 - accuracy: 0.7712 - val_loss: 0.5352 - val_accuracy: 0.7993\n",
      "Q 268+118 T 386  ☒ 385 \n",
      "Q 70+962  T 1032 ☒ 1023\n",
      "Q 44+53   T 97   ☒ 99  \n",
      "Q 8+165   T 173  ☒ 172 \n",
      "Q 129+837 T 966  ☒ 965 \n",
      "Q 332+135 T 467  ☒ 466 \n",
      "Q 533+878 T 1411 ☑ 1411\n",
      "Q 68+328  T 396  ☑ 396 \n",
      "Q 33+12   T 45   ☒ 46  \n",
      "Q 53+571  T 624  ☒ 625 \n",
      "\n",
      "Iteration 7\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.3848 - accuracy: 0.8705 - val_loss: 0.2837 - val_accuracy: 0.9184\n",
      "Q 15+523  T 538  ☑ 538 \n",
      "Q 835+63  T 898  ☑ 898 \n",
      "Q 535+564 T 1099 ☒ 1199\n",
      "Q 228+48  T 276  ☒ 275 \n",
      "Q 457+26  T 483  ☑ 483 \n",
      "Q 127+1   T 128  ☑ 128 \n",
      "Q 287+568 T 855  ☒ 856 \n",
      "Q 519+11  T 530  ☑ 530 \n",
      "Q 361+20  T 381  ☒ 382 \n",
      "Q 252+344 T 596  ☑ 596 \n",
      "\n",
      "Iteration 8\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.2205 - accuracy: 0.9399 - val_loss: 0.1716 - val_accuracy: 0.9556\n",
      "Q 42+512  T 554  ☑ 554 \n",
      "Q 821+7   T 828  ☑ 828 \n",
      "Q 237+695 T 932  ☑ 932 \n",
      "Q 881+71  T 952  ☑ 952 \n",
      "Q 665+827 T 1492 ☑ 1492\n",
      "Q 476+450 T 926  ☑ 926 \n",
      "Q 537+919 T 1456 ☑ 1456\n",
      "Q 701+3   T 704  ☑ 704 \n",
      "Q 362+940 T 1302 ☒ 1301\n",
      "Q 534+756 T 1290 ☑ 1290\n",
      "\n",
      "Iteration 9\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1430 - accuracy: 0.9638 - val_loss: 0.1577 - val_accuracy: 0.9512\n",
      "Q 766+380 T 1146 ☑ 1146\n",
      "Q 88+14   T 102  ☑ 102 \n",
      "Q 596+94  T 690  ☑ 690 \n",
      "Q 52+407  T 459  ☑ 459 \n",
      "Q 340+70  T 410  ☑ 410 \n",
      "Q 422+13  T 435  ☒ 436 \n",
      "Q 982+81  T 1063 ☑ 1063\n",
      "Q 496+1   T 497  ☑ 497 \n",
      "Q 56+72   T 128  ☑ 128 \n",
      "Q 562+505 T 1067 ☑ 1067\n",
      "\n",
      "Iteration 10\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0892 - accuracy: 0.9805 - val_loss: 0.0693 - val_accuracy: 0.9872\n",
      "Q 880+4   T 884  ☑ 884 \n",
      "Q 601+8   T 609  ☑ 609 \n",
      "Q 888+1   T 889  ☑ 889 \n",
      "Q 546+477 T 1023 ☑ 1023\n",
      "Q 551+81  T 632  ☑ 632 \n",
      "Q 5+810   T 815  ☑ 815 \n",
      "Q 1+81    T 82   ☑ 82  \n",
      "Q 979+44  T 1023 ☑ 1023\n",
      "Q 337+894 T 1231 ☑ 1231\n",
      "Q 512+133 T 645  ☑ 645 \n",
      "\n",
      "Iteration 11\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0802 - accuracy: 0.9803 - val_loss: 0.0523 - val_accuracy: 0.9887\n",
      "Q 241+27  T 268  ☑ 268 \n",
      "Q 81+506  T 587  ☑ 587 \n",
      "Q 390+32  T 422  ☑ 422 \n",
      "Q 70+987  T 1057 ☑ 1057\n",
      "Q 78+286  T 364  ☑ 364 \n",
      "Q 231+42  T 273  ☑ 273 \n",
      "Q 719+991 T 1710 ☑ 1710\n",
      "Q 856+21  T 877  ☑ 877 \n",
      "Q 337+894 T 1231 ☑ 1231\n",
      "Q 64+183  T 247  ☑ 247 \n",
      "\n",
      "Iteration 12\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0518 - accuracy: 0.9878 - val_loss: 0.0668 - val_accuracy: 0.9814\n",
      "Q 61+852  T 913  ☑ 913 \n",
      "Q 137+2   T 139  ☑ 139 \n",
      "Q 104+76  T 180  ☑ 180 \n",
      "Q 320+996 T 1316 ☑ 1316\n",
      "Q 70+30   T 100  ☑ 100 \n",
      "Q 6+836   T 842  ☑ 842 \n",
      "Q 788+421 T 1209 ☒ 1109\n",
      "Q 27+721  T 748  ☑ 748 \n",
      "Q 30+424  T 454  ☑ 454 \n",
      "Q 29+732  T 761  ☑ 761 \n",
      "\n",
      "Iteration 13\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0567 - accuracy: 0.9853 - val_loss: 0.0889 - val_accuracy: 0.9685\n",
      "Q 293+445 T 738  ☒ 739 \n",
      "Q 50+751  T 801  ☒ 811 \n",
      "Q 163+99  T 262  ☑ 262 \n",
      "Q 15+909  T 924  ☑ 924 \n",
      "Q 93+198  T 291  ☑ 291 \n",
      "Q 20+71   T 91   ☑ 91  \n",
      "Q 66+796  T 862  ☑ 862 \n",
      "Q 213+38  T 251  ☑ 251 \n",
      "Q 23+37   T 60   ☑ 60  \n",
      "Q 911+490 T 1401 ☑ 1401\n",
      "\n",
      "Iteration 14\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0417 - accuracy: 0.9893 - val_loss: 0.0664 - val_accuracy: 0.9790\n",
      "Q 815+209 T 1024 ☑ 1024\n",
      "Q 19+359  T 378  ☑ 378 \n",
      "Q 143+112 T 255  ☑ 255 \n",
      "Q 189+6   T 195  ☑ 195 \n",
      "Q 24+642  T 666  ☑ 666 \n",
      "Q 52+522  T 574  ☑ 574 \n",
      "Q 389+5   T 394  ☑ 394 \n",
      "Q 894+10  T 904  ☑ 904 \n",
      "Q 85+12   T 97   ☒ 98  \n",
      "Q 409+35  T 444  ☑ 444 \n",
      "\n",
      "Iteration 15\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0253 - accuracy: 0.9948 - val_loss: 0.0803 - val_accuracy: 0.9745\n",
      "Q 968+4   T 972  ☑ 972 \n",
      "Q 99+47   T 146  ☒ 136 \n",
      "Q 15+11   T 26   ☑ 26  \n",
      "Q 46+446  T 492  ☑ 492 \n",
      "Q 414+941 T 1355 ☑ 1355\n",
      "Q 346+445 T 791  ☑ 791 \n",
      "Q 0+510   T 510  ☑ 510 \n",
      "Q 478+176 T 654  ☑ 654 \n",
      "Q 623+632 T 1255 ☑ 1255\n",
      "Q 871+14  T 885  ☑ 885 \n",
      "\n",
      "Iteration 16\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0421 - accuracy: 0.9882 - val_loss: 0.0150 - val_accuracy: 0.9971\n",
      "Q 787+732 T 1519 ☑ 1519\n",
      "Q 72+18   T 90   ☑ 90  \n",
      "Q 719+525 T 1244 ☑ 1244\n",
      "Q 23+790  T 813  ☑ 813 \n",
      "Q 989+240 T 1229 ☑ 1229\n",
      "Q 59+25   T 84   ☑ 84  \n",
      "Q 39+812  T 851  ☑ 851 \n",
      "Q 59+714  T 773  ☑ 773 \n",
      "Q 87+65   T 152  ☑ 152 \n",
      "Q 622+153 T 775  ☑ 775 \n",
      "\n",
      "Iteration 17\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0426 - accuracy: 0.9882 - val_loss: 0.0150 - val_accuracy: 0.9973\n",
      "Q 278+828 T 1106 ☑ 1106\n",
      "Q 45+322  T 367  ☑ 367 \n",
      "Q 30+185  T 215  ☑ 215 \n",
      "Q 978+215 T 1193 ☑ 1193\n",
      "Q 160+9   T 169  ☑ 169 \n",
      "Q 834+68  T 902  ☑ 902 \n",
      "Q 174+25  T 199  ☑ 199 \n",
      "Q 424+421 T 845  ☑ 845 \n",
      "Q 529+618 T 1147 ☒ 1247\n",
      "Q 917+193 T 1110 ☑ 1110\n",
      "\n",
      "Iteration 18\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0169 - accuracy: 0.9965 - val_loss: 0.0940 - val_accuracy: 0.9679\n",
      "Q 610+988 T 1598 ☑ 1598\n",
      "Q 311+9   T 320  ☑ 320 \n",
      "Q 715+510 T 1225 ☑ 1225\n",
      "Q 369+60  T 429  ☑ 429 \n",
      "Q 754+725 T 1479 ☑ 1479\n",
      "Q 90+753  T 843  ☒ 844 \n",
      "Q 753+975 T 1728 ☑ 1728\n",
      "Q 850+375 T 1225 ☑ 1225\n",
      "Q 833+882 T 1715 ☑ 1715\n",
      "Q 416+967 T 1383 ☑ 1383\n",
      "\n",
      "Iteration 19\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0395 - accuracy: 0.9883 - val_loss: 0.0208 - val_accuracy: 0.9950\n",
      "Q 31+64   T 95   ☑ 95  \n",
      "Q 135+13  T 148  ☑ 148 \n",
      "Q 1+828   T 829  ☑ 829 \n",
      "Q 1+996   T 997  ☑ 997 \n",
      "Q 36+242  T 278  ☑ 278 \n",
      "Q 658+500 T 1158 ☑ 1158\n",
      "Q 257+705 T 962  ☑ 962 \n",
      "Q 60+266  T 326  ☑ 326 \n",
      "Q 5+934   T 939  ☑ 939 \n",
      "Q 43+982  T 1025 ☑ 1025\n",
      "\n",
      "Iteration 20\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0292 - accuracy: 0.9918 - val_loss: 0.0279 - val_accuracy: 0.9924\n",
      "Q 2+728   T 730  ☑ 730 \n",
      "Q 110+206 T 316  ☑ 316 \n",
      "Q 251+24  T 275  ☑ 275 \n",
      "Q 369+225 T 594  ☑ 594 \n",
      "Q 851+0   T 851  ☑ 851 \n",
      "Q 766+66  T 832  ☑ 832 \n",
      "Q 733+577 T 1310 ☑ 1310\n",
      "Q 2+440   T 442  ☑ 442 \n",
      "Q 194+27  T 221  ☑ 221 \n",
      "Q 61+537  T 598  ☑ 598 \n",
      "\n",
      "Iteration 21\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0157 - accuracy: 0.9960 - val_loss: 0.0291 - val_accuracy: 0.9905\n",
      "Q 287+804 T 1091 ☑ 1091\n",
      "Q 117+882 T 999  ☑ 999 \n",
      "Q 1+83    T 84   ☑ 84  \n",
      "Q 55+63   T 118  ☑ 118 \n",
      "Q 196+944 T 1140 ☑ 1140\n",
      "Q 141+7   T 148  ☑ 148 \n",
      "Q 518+5   T 523  ☑ 523 \n",
      "Q 461+9   T 470  ☑ 470 \n",
      "Q 86+613  T 699  ☑ 699 \n",
      "Q 286+421 T 707  ☑ 707 \n",
      "\n",
      "Iteration 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0116 - accuracy: 0.9974 - val_loss: 0.0448 - val_accuracy: 0.9868\n",
      "Q 348+482 T 830  ☑ 830 \n",
      "Q 2+610   T 612  ☑ 612 \n",
      "Q 75+20   T 95   ☑ 95  \n",
      "Q 387+444 T 831  ☑ 831 \n",
      "Q 293+445 T 738  ☑ 738 \n",
      "Q 620+545 T 1165 ☑ 1165\n",
      "Q 835+57  T 892  ☑ 892 \n",
      "Q 386+91  T 477  ☑ 477 \n",
      "Q 964+11  T 975  ☑ 975 \n",
      "Q 681+686 T 1367 ☑ 1367\n",
      "\n",
      "Iteration 23\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0255 - accuracy: 0.9927 - val_loss: 0.0048 - val_accuracy: 0.9992\n",
      "Q 254+803 T 1057 ☑ 1057\n",
      "Q 615+69  T 684  ☑ 684 \n",
      "Q 804+64  T 868  ☑ 868 \n",
      "Q 696+167 T 863  ☑ 863 \n",
      "Q 750+61  T 811  ☑ 811 \n",
      "Q 45+986  T 1031 ☑ 1031\n",
      "Q 528+481 T 1009 ☑ 1009\n",
      "Q 825+832 T 1657 ☑ 1657\n",
      "Q 529+859 T 1388 ☑ 1388\n",
      "Q 44+134  T 178  ☑ 178 \n",
      "\n",
      "Iteration 24\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0233 - accuracy: 0.9931 - val_loss: 0.0191 - val_accuracy: 0.9943\n",
      "Q 91+798  T 889  ☑ 889 \n",
      "Q 172+642 T 814  ☑ 814 \n",
      "Q 719+365 T 1084 ☑ 1084\n",
      "Q 6+751   T 757  ☑ 757 \n",
      "Q 42+531  T 573  ☑ 573 \n",
      "Q 19+489  T 508  ☑ 508 \n",
      "Q 140+747 T 887  ☑ 887 \n",
      "Q 338+35  T 373  ☑ 373 \n",
      "Q 0+537   T 537  ☑ 537 \n",
      "Q 936+681 T 1617 ☑ 1617\n",
      "\n",
      "Iteration 25\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0176 - accuracy: 0.9953 - val_loss: 0.0067 - val_accuracy: 0.9985\n",
      "Q 79+843  T 922  ☑ 922 \n",
      "Q 6+836   T 842  ☑ 842 \n",
      "Q 6+332   T 338  ☑ 338 \n",
      "Q 524+54  T 578  ☑ 578 \n",
      "Q 544+821 T 1365 ☑ 1365\n",
      "Q 606+236 T 842  ☑ 842 \n",
      "Q 709+47  T 756  ☑ 756 \n",
      "Q 355+70  T 425  ☑ 425 \n",
      "Q 843+991 T 1834 ☑ 1834\n",
      "Q 59+308  T 367  ☑ 367 \n",
      "\n",
      "Iteration 26\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0202 - accuracy: 0.9945 - val_loss: 0.0122 - val_accuracy: 0.9974\n",
      "Q 76+528  T 604  ☑ 604 \n",
      "Q 396+824 T 1220 ☑ 1220\n",
      "Q 7+562   T 569  ☑ 569 \n",
      "Q 29+707  T 736  ☑ 736 \n",
      "Q 27+19   T 46   ☑ 46  \n",
      "Q 457+26  T 483  ☑ 483 \n",
      "Q 823+875 T 1698 ☑ 1698\n",
      "Q 412+804 T 1216 ☑ 1216\n",
      "Q 60+21   T 81   ☑ 81  \n",
      "Q 98+66   T 164  ☑ 164 \n",
      "\n",
      "Iteration 27\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.0108 - val_accuracy: 0.9977\n",
      "Q 669+78  T 747  ☑ 747 \n",
      "Q 48+61   T 109  ☑ 109 \n",
      "Q 310+11  T 321  ☑ 321 \n",
      "Q 88+904  T 992  ☑ 992 \n",
      "Q 137+2   T 139  ☑ 139 \n",
      "Q 509+849 T 1358 ☑ 1358\n",
      "Q 732+32  T 764  ☑ 764 \n",
      "Q 56+97   T 153  ☑ 153 \n",
      "Q 23+658  T 681  ☑ 681 \n",
      "Q 91+3    T 94   ☑ 94  \n",
      "\n",
      "Iteration 28\n",
      "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0979 - val_accuracy: 0.9691\n",
      "Q 52+43   T 95   ☑ 95  \n",
      "Q 424+3   T 427  ☑ 427 \n",
      "Q 875+74  T 949  ☑ 949 \n",
      "Q 83+97   T 180  ☑ 180 \n",
      "Q 674+378 T 1052 ☒ 1053\n",
      "Q 4+755   T 759  ☑ 759 \n",
      "Q 78+47   T 125  ☒ 126 \n",
      "Q 61+738  T 799  ☑ 799 \n",
      "Q 868+73  T 941  ☑ 941 \n",
      "Q 48+61   T 109  ☑ 109 \n",
      "\n",
      "Iteration 29\n",
      "1407/1407 [==============================] - 8s 5ms/step - loss: 0.0193 - accuracy: 0.9947 - val_loss: 0.0050 - val_accuracy: 0.9989\n",
      "Q 13+327  T 340  ☑ 340 \n",
      "Q 686+91  T 777  ☑ 777 \n",
      "Q 177+1   T 178  ☑ 178 \n",
      "Q 47+109  T 156  ☑ 156 \n",
      "Q 866+35  T 901  ☑ 901 \n",
      "Q 57+485  T 542  ☑ 542 \n",
      "Q 285+94  T 379  ☑ 379 \n",
      "Q 517+199 T 716  ☑ 716 \n",
      "Q 295+885 T 1180 ☑ 1180\n",
      "Q 70+85   T 155  ☑ 155 \n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for epoch in range(1, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
